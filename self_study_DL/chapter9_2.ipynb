{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackJJW/DL/blob/main/self_study_DL/chapter9_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F99QK_Llh1IJ"
      },
      "source": [
        "# 09-2 순환 신경망으로 IMDB 리뷰 분류하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CMH3Y5BiJu2"
      },
      "source": [
        "## IMDB 리뷰 데이터 셋"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJNZh-noiQIK"
      },
      "source": [
        "- IMDB 리뷰 데이터셋은 imdb.com에서 수집한 리뷰 감상평에 따라 긍정과 부정으로 분류해 놓은 데이터셋\n",
        "- 총 50,000개의 샘플\n",
        "- 훈련 데이터와 테스트 데이터 각각 25,000개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_mE6qE26pqV"
      },
      "source": [
        "> 자연어 처리와 말뭉치\n",
        "- **자연어 처리**(natural language processing, NLP) : 컴퓨터를 사용해 인간의 언어를 처리하는 분야\n",
        "  - ex) 음성 인식, 기계 번역, 감성 분석 등\n",
        "- **말뭉치**(corpus) : 자연어 처리 분야에서의 훈련 데이터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2kX-NN17chZ"
      },
      "source": [
        "- 텍스트 자체를 신경망에 전달하지 않은.\n",
        "  - 컴퓨터에서 처리하는 모든 것은 어떤 숫자 데이터\n",
        "  - 텍스트 데이터의 경우 단어를 숫자 데이터로 바꾸는 방법 : 단어마다 고유한 정수를 부여\n",
        "- **토큰**(token) : 영어 문장을 모두 소문자로 바꾸고 구둣점을 삭제한 다음 공백을 기준으로 분리한 단어\n",
        "  - 하나의 샘플은 여러 개의 토큰으로 이루어져 있고 1개의 토큰이 하나의 타임스텝에 해당"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWXxMQ0w9Hbu"
      },
      "source": [
        "> 한글 문장은 어떻게 토큰을 분리?\n",
        "- 한글은 조사가 발달, 공백으로 나누는 것만으로는 부족\n",
        "- 일반적으로 한글은 형태소 분석을 통해 토큰을 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve07txA3DjcJ"
      },
      "source": [
        "- 토큰에 할당하는 정수 중에 몇 개는 특정한 용도로 예약\n",
        "  - 0 : 패딩\n",
        "  - 1 : 문장의 시작\n",
        "  - 2 : 어휘 사전에 없는 토큰\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_QCO9HDD-pI"
      },
      "source": [
        "> 어휘 사전\n",
        "- 훈련 세트에서 고유한 단어를 뽑아 만든 목록\n",
        "- ex) 테스트 세트 안에 어휘 사전에 없는 단어가 았다면 2로 변환하여 신경망 모델에 주입"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fAd5Rn7Fg0r"
      },
      "source": [
        "- tensorflow.keras.datasets 패키지 아래 imdb 모듈을 임포트\n",
        "- 가장 자주 등장하는 단어 500개"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH3J67pkFtvh",
        "outputId": "42685e5c-a050-4dac-c813-5e8447d2d5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdDTGuJ_GJjU",
        "outputId": "65fb6777-f7ce-4bbd-9876-ad70ac62dd49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000,) (25000,)\n"
          ]
        }
      ],
      "source": [
        "print(train_input.shape, test_input.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOOMfPTJGUwb"
      },
      "source": [
        "- IMDB 리뷰 텍스트는 길이가 제각각\n",
        "- 고정 크기의 2차원 배열에 담기 보다는 리뷰마다 별도의 파이썬 리스트로 담아야 메모리를 효율적으로 사용 가능\n",
        "\n",
        "\n",
        "```\n",
        "train_input: [리뷰1, 리뷰2, 리뷰3, ...] <- 넘파이 배열\n",
        "```\n",
        "- 이 데이터는 개별 리뷰를 담은 파이썬 리스트 객체로 이루어진 넘파이 배열\n",
        "- 넘파이 배열은 정수나 실수 외에도 파이썬 객체를 담을 수 있음\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqo42xV6HISf",
        "outputId": "6bcc6012-2da4-4e50-d1b2-eb6cf8a16c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "218\n"
          ]
        }
      ],
      "source": [
        "print(len(train_input[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQQAGkmkHPuh",
        "outputId": "dd7adcf6-0dc8-4bcd-ec9d-8c2cbd857ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "189\n"
          ]
        }
      ],
      "source": [
        "print(len(train_input[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WM1rVvYHU6X",
        "outputId": "5d28f6c0-2f6c-4752-d716-b7e66c27b0cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
          ]
        }
      ],
      "source": [
        "print(train_input[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QawuXZHHOy7"
      },
      "source": [
        "- 텐서플로에 있는 IMDB 리뷰 데이터는 이미 정수로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2kh774FHnMo",
        "outputId": "5533a69f-fd78-4daa-d770-2870649b4482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1]\n"
          ]
        }
      ],
      "source": [
        "print(train_target[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOJ1lpUOHs8P"
      },
      "source": [
        "- 해결할 문제는 리뷰가 긍정인지 부정인지 판단하는 것\n",
        "  - 이진 분류 문제로 볼 수 있으며 타깃값이 0(부정)과 1(긍정)로 나누어짐"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bd8Hg1XiIuHe"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 각 리뷰의 길이를 계산하여 넘파이 배열에 담음\n",
        "  - 평균적인 리뷰의 길이와 가장 짧은 리뷰의 길이 그리고 가장 긴 리뷰의 길이를 확인 하고자 함"
      ],
      "metadata": {
        "id": "BmLDazj2JOVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "lengths = np.array([len(x) for x in train_input])"
      ],
      "metadata": {
        "id": "A5j0xMlrJjcQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(lengths), np.median(lengths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4fZqetLJ4_Y",
        "outputId": "7cc2d6fd-cb19-401b-f8f6-a20fd8100dbb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "239.00925 178.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(lengths)\n",
        "plt.xlabel('lengths')\n",
        "plt.ylabel('frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "2fcxbwL2KEfs",
        "outputId": "a0604539-927e-44dd-e8cd-d25a1e58dc23"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWuElEQVR4nO3de7SldX3f8fdHUKJ4AWTKwoFmxogxaBPEKVCNrq5guapDrRdctk4ILY3FRtumyRC7xHhJIEattFGLAQMGBeqlzAoanKImq10BOQPIVeSIIOAAo8NNbYyD3/7x/A5uxnOGPc+cvffZOe/XWnvt5/k9t+/znDnzOc89VYUkSX08YdIFSJKmlyEiSerNEJEk9WaISJJ6M0QkSb3tPukCxm3fffetVatWTboMSZoamzZt+m5VrZhv2LILkVWrVjEzMzPpMiRpaiS5Y6FhHs6SJPVmiEiSejNEJEm9GSKSpN4MEUlSb4aIJKk3Q0SS1JshIknqzRCRJPW27O5Y3xWr1l86keXefsbxE1muJD0e90QkSb0ZIpKk3gwRSVJvhogkqTdDRJLUmyEiSerNEJEk9WaISJJ6M0QkSb0ZIpKk3gwRSVJvhogkqTdDRJLUmyEiSerNEJEk9WaISJJ6M0QkSb2NLESSnJvkviQ3DLTtk2Rjklvb996tPUnOSjKb5Lokhw5Ms66Nf2uSdQPtL0pyfZvmrCQZ1bpIkuY3yj2RPwOO2a5tPXB5VR0EXN76AY4FDmqfU4CPQBc6wOnA4cBhwOlzwdPG+TcD022/LEnSiI0sRKrqr4Gt2zWvBc5r3ecBJwy0n1+dK4C9kuwPHA1srKqtVXU/sBE4pg17elVdUVUFnD8wL0nSmIz7nMh+VbW5dd8D7Ne6VwJ3Dox3V2vbUftd87TPK8kpSWaSzGzZsmXX1kCS9KiJnVhvexA1pmWdXVVrqmrNihUrxrFISVoWxh0i97ZDUbTv+1r73cCBA+Md0Np21H7APO2SpDEad4hsAOausFoHXDLQ/qZ2ldYRwIPtsNdlwFFJ9m4n1I8CLmvDHkpyRLsq600D85Ikjcnuo5pxkk8B/xTYN8lddFdZnQFcnORk4A7gdW30zwPHAbPAD4GTAKpqa5J3A1e18d5VVXMn6/8d3RVgTwa+0D6SpDEaWYhU1RsWGHTkPOMWcOoC8zkXOHee9hngBbtSoyRp13jHuiSpN0NEktSbISJJ6s0QkST1ZohIknozRCRJvRkikqTeDBFJUm+GiCSpN0NEktSbISJJ6s0QkST1ZohIknozRCRJvRkikqTeDBFJUm+GiCSpN0NEktSbISJJ6s0QkST1ZohIknozRCRJvRkikqTeDBFJUm+GiCSpN0NEktSbISJJ6s0QkST1NpEQSfIfktyY5IYkn0ryc0lWJ7kyyWySi5I8qY27R+ufbcNXDczntNZ+S5KjJ7EukrScjT1EkqwEfgtYU1UvAHYDTgTOBD5YVc8B7gdObpOcDNzf2j/YxiPJwW265wPHAB9Osts410WSlrtJHc7aHXhykt2BpwCbgV8DPt2Gnwec0LrXtn7a8COTpLVfWFU/qqpvAbPAYWOqX5LEBEKkqu4G/hj4Nl14PAhsAh6oqm1ttLuAla17JXBnm3ZbG/+Zg+3zTPMYSU5JMpNkZsuWLYu7QpK0jE3icNbedHsRq4FnAXvSHY4amao6u6rWVNWaFStWjHJRkrSsTOJw1suBb1XVlqr6MfBZ4CXAXu3wFsABwN2t+27gQIA2/BnA9wbb55lGkjQGkwiRbwNHJHlKO7dxJHAT8GXgNW2cdcAlrXtD66cN/1JVVWs/sV29tRo4CPjqmNZBkkR3gnusqurKJJ8Grga2AdcAZwOXAhcmeU9rO6dNcg7wiSSzwFa6K7KoqhuTXEwXQNuAU6vqkbGujCQtc2MPEYCqOh04fbvm25jn6qqq+lvgtQvM573Aexe9QEnSULxjXZLUmyEiSerNEJEk9WaISJJ6M0QkSb0ZIpKk3gwRSVJvhogkqTdDRJLUmyEiSerNEJEk9WaISJJ6e9wQSbIpyantZVKSJD1qmD2R19O9gfCqJBcmObq9B0SStMw9bohU1WxVvR14LvBJ4FzgjiS/n2SfURcoSVq6hjonkuSXgfcD7wM+Q/d+j4eAL42uNEnSUve4L6VKsgl4gO4Ng+ur6kdt0JVJXjLK4iRJS9swbzZ8bVXdNt+Aqnr1ItcjSZoiwxzO+tdJ9prrSbJ3ew+6JGmZGyZEjq2qB+Z6qup+4LjRlSRJmhbDhMhuSfaY60nyZGCPHYwvSVomhjkncgFweZKPt/6TgPNGV5IkaVo8bohU1ZlJrgOObE3vrqrLRluWJGkaDLMnQlV9AfjCiGuRJE2ZYZ6d9eoktyZ5MMlDSR5O8tA4ipMkLW3D7In8EfDKqrp51MVIkqbLMFdn3WuASJLmM8yeyEySi4D/Bcw98oSq+uzIqpIkTYVh9kSeDvwQOAp4Zfu8YlcWmmSvJJ9O8vUkNyf5J0n2SbKxnX/ZOPf+knTOSjKb5Lokhw7MZ10b/9Yk63alJknSzhvmEt+TRrDcDwF/WVWvSfIk4CnA7wGXV9UZSdYD64HfBY4FDmqfw4GPAIe3x9CfDqwBCtiUZEO7o16SNAbDXJ313CSXJ7mh9f9ykv/Sd4FJngG8jO6pwFTV37XHqqzlpzcxngec0LrXAudX5wpgryT7A0cDG6tqawuOjcAxfeuSJO28YQ5nfQw4DfgxQFVdB5y4C8tcDWwBPp7kmiR/mmRPYL+q2tzGuQfYr3WvBO4cmP6u1rZQ+89IckqSmSQzW7Zs2YXSJUmDhgmRp1TVV7dr27YLy9wdOBT4SFW9EPgB3aGrR1VV0R2iWhRVdXZVramqNStWrFis2UrSsjdMiHw3yS/Q/lNP8hpg844n2aG7gLuq6srW/2m6ULm3Haaifd/Xht8NHDgw/QGtbaF2SdKYDBMipwL/A3hekruBtwFv7rvAqroHuDPJL7amI4GbgA3A3BVW64BLWvcG4E3tKq0jgAfbYa/LgKPa+032prt6zGd6SdIYDXN11m3Ay9t5iydU1cOLsNx/D1zQrsy6je7JwE8ALk5yMnAH8Lo27ufp3l8yS3ep8Umtrq1J3g1c1cZ7V1VtXYTaJElDSnf6YQcjJO+Yr72q3jWSikZszZo1NTMz02vaVesvXeRqlr7bzzh+0iVImrAkm6pqzXzDhrlj/QcD3T9Hd6Ohj0GRJA11OOv9g/1J/hjPPUiSGO7E+vaeQncllCRpmXvcPZEk1/PTezZ2A1YAU3k+RJK0uIY5JzL4sMVtdI+G35WbDSVJf08MEyLbX9L79CSP9nhZrSQtX8OEyNV0d4bfDwTYC/h2G1bAs0dTmiRpqRvmxPpGutfj7ltVz6Q7vPXFqlpdVQaIJC1jw4TIEVX1+bmeqvoC8OLRlSRJmhbDHM76Tnt/yJ+3/jcC3xldSZKkaTHMnsgb6C7r/Rzw2db9hlEWJUmaDsPcsb4VeGuSPavqB483viRp+Rjm9bgvTnIT7XlZSX4lyYdHXpkkackb5nDWB+neZ/49gKr6Gt070iVJy9xQz86qqju3a3pkBLVIkqbMMFdn3ZnkxUAleSLwVnwUvCSJ4fZEfpPuFbkr6d5hfkjrlyQtczvcE0myG/ChqnrjmOqRJE2RHe6JVNUjwM+3d6FLkvQYw5wTuQ34v0k2MPCq3Kr6wMiqkiRNhQX3RJJ8onW+CviLNu7TBj6SpGVuR3siL0ryLLrHvv+3MdUjSZoiOwqRjwKXA6uBmYH24HtEJEns4HBWVZ1VVb8EfLyqnj3w8T0ikiRgiPtEqurN4yhEkjR9hnrsiSRJ8zFEJEm9GSKSpN4mFiJJdktyTZK/aP2rk1yZZDbJRXN3ySfZo/XPtuGrBuZxWmu/JcnRk1kTSVq+Jrknsv3TgM8EPlhVzwHuB05u7ScD97f2D7bxSHIwcCLwfOAY4MPtWV+SpDGZSIgkOQA4HvjT1h/g14BPt1HOA05o3WtbP234kW38tcCFVfWjqvoWMAscNp41kCTB5PZE/ivwO8BPWv8zgQeqalvrv4vu0fO07zsB2vAH2/iPts8zjSRpDMYeIkleAdxXVZvGuMxTkswkmdmyZcu4FitJf+9NYk/kJcCrktwOXEh3GOtDwF5J5h7DcgDdC7Bo3wcCtOHPoHvf+6Pt80zzGFV1dlWtqao1K1asWNy1kaRlbOwhUlWnVdUBVbWK7sT4l9pLr74MvKaNtg64pHVvaP204V+qqmrtJ7art1YDBwFfHdNqSJIY7n0i4/K7wIVJ3gNcA5zT2s8BPpFkFthKFzxU1Y1JLgZuArYBp7aXaEmSxmSiIVJVXwG+0rpvY56rq6rqb4HXLjD9e4H3jq5CSdKOeMe6JKk3Q0SS1JshIknqzRCRJPVmiEiSejNEJEm9GSKSpN4MEUlSb4aIJKk3Q0SS1JshIknqzRCRJPVmiEiSejNEJEm9GSKSpN4MEUlSb4aIJKk3Q0SS1JshIknqzRCRJPVmiEiSejNEJEm9GSKSpN4MEUlSb4aIJKk3Q0SS1JshIknqzRCRJPU29hBJcmCSLye5KcmNSd7a2vdJsjHJre1779aeJGclmU1yXZJDB+a1ro1/a5J1414XSVruJrEnsg34T1V1MHAEcGqSg4H1wOVVdRBweesHOBY4qH1OAT4CXegApwOHA4cBp88FjyRpPMYeIlW1uaqubt0PAzcDK4G1wHlttPOAE1r3WuD86lwB7JVkf+BoYGNVba2q+4GNwDFjXBVJWvYmek4kySrghcCVwH5VtbkNugfYr3WvBO4cmOyu1rZQuyRpTCYWIkmeCnwGeFtVPTQ4rKoKqEVc1ilJZpLMbNmyZbFmK0nL3kRCJMkT6QLkgqr6bGu+tx2mon3f19rvBg4cmPyA1rZQ+8+oqrOrak1VrVmxYsXirYgkLXO7j3uBSQKcA9xcVR8YGLQBWAec0b4vGWh/S5IL6U6iP1hVm5NcBvzBwMn0o4DTxrEOy8mq9ZdOZLm3n3H8RJYraeeMPUSAlwD/Crg+ybWt7ffowuPiJCcDdwCva8M+DxwHzAI/BE4CqKqtSd4NXNXGe1dVbR3PKkiSYAIhUlX/B8gCg4+cZ/wCTl1gXucC5y5edZKkneEd65Kk3gwRSVJvhogkqTdDRJLUmyEiSerNEJEk9WaISJJ6M0QkSb0ZIpKk3gwRSVJvhogkqTdDRJLUmyEiSerNEJEk9WaISJJ6M0QkSb0ZIpKk3gwRSVJvk3jHuvS4Vq2/dGLLvv2M4ye2bGnauCciSerNEJEk9WaISJJ6M0QkSb0ZIpKk3gwRSVJvhogkqTfvE5G2M6l7VLw/RdPIPRFJUm/uiUhLhHtAmkZTvyeS5JgktySZTbJ+0vVI0nIy1SGSZDfgT4BjgYOBNyQ5eLJVSdLyMdUhAhwGzFbVbVX1d8CFwNoJ1yRJy8a0nxNZCdw50H8XcPj2IyU5BTil9X4/yS07uZx9ge/2qnB8pqFGmI46l1WNOXMx5jKvadiOMB11TrrGn19owLSHyFCq6mzg7L7TJ5mpqjWLWNKim4YaYTrqtMbFMQ01wnTUuZRrnPbDWXcDBw70H9DaJEljMO0hchVwUJLVSZ4EnAhsmHBNkrRsTPXhrKraluQtwGXAbsC5VXXjCBbV+1DYGE1DjTAddVrj4piGGmE66lyyNaaqJl2DJGlKTfvhLEnSBBkikqTeDJEdWEqPVElyYJIvJ7kpyY1J3tra35nk7iTXts9xA9Oc1mq/JcnRY6rz9iTXt1pmWts+STYmubV9793ak+SsVuN1SQ4dQ32/OLCtrk3yUJK3LYXtmOTcJPcluWGgbae3XZJ1bfxbk6wbQ43vS/L1VsfnkuzV2lcl+X8D2/SjA9O8qP07mW3rkRHXuNM/31H+/i9Q40UD9d2e5NrWPpHtOLSq8jPPh+5E/TeBZwNPAr4GHDzBevYHDm3dTwO+Qfeol3cCvz3P+Ae3mvcAVrd12W0Mdd4O7Ltd2x8B61v3euDM1n0c8AUgwBHAlRP4Gd9DdyPVxLcj8DLgUOCGvtsO2Ae4rX3v3br3HnGNRwG7t+4zB2pcNTjedvP5aqs7bT2OHXGNO/XzHfXv/3w1bjf8/cA7Jrkdh/24J7KwJfVIlaraXFVXt+6HgZvp7thfyFrgwqr6UVV9C5ilW6dJWAuc17rPA04YaD+/OlcAeyXZf4x1HQl8s6ru2ME4Y9uOVfXXwNZ5lr8z2+5oYGNVba2q+4GNwDGjrLGqvlhV21rvFXT3ay2o1fn0qrqiuv8Jzx9Yr5HUuAML/XxH+vu/oxrb3sTrgE/taB6j3o7DMkQWNt8jVXb0n/bYJFkFvBC4sjW9pR1KOHfucAeTq7+ALybZlO5xMwD7VdXm1n0PsN+Ea5xzIo/9RV1K23HOzm67Sdf7G3R/Ec9ZneSaJH+V5KWtbWWra864atyZn+8kt+NLgXur6taBtqW0HR/DEJkySZ4KfAZ4W1U9BHwE+AXgEGAz3W7wJP1qVR1K92TlU5O8bHBg+4tp4teVp7s59VXA/2xNS207/oylsu0WkuTtwDbggta0GfiHVfVC4D8Cn0zy9AmVt+R/vgPewGP/uFlK2/FnGCILW3KPVEnyRLoAuaCqPgtQVfdW1SNV9RPgY/z0UMtE6q+qu9v3fcDnWj33zh2mat/3TbLG5ljg6qq6t9W7pLbjgJ3ddhOpN8mvA68A3tjCjnaI6HutexPdOYbntnoGD3mNvMYeP99JbcfdgVcDF821LaXtOB9DZGFL6pEq7TjpOcDNVfWBgfbBcwj/HJi72mMDcGKSPZKsBg6iOwk3yhr3TPK0uW66E643tFrmrhJaB1wyUOOb2pVGRwAPDhy6GbXH/LW3lLbjdnZ2210GHJVk73bI5qjWNjJJjgF+B3hVVf1woH1Funf+kOTZdNvutlbnQ0mOaP+u3zSwXqOqcWd/vpP6/X858PWqevQw1VLajvMa95n8afrQXQHzDbrkf/uEa/lVukMZ1wHXts9xwCeA61v7BmD/gWne3mq/hTFctUF3JcvX2ufGuW0GPBO4HLgV+N/APq09dC8V+2ZbhzVj2pZ7At8DnjHQNvHtSBdqm4Ef0x3fPrnPtqM7LzHbPieNocZZuvMHc/8uP9rG/Rft38G1wNXAKwfms4buP/JvAv+d9vSMEda40z/fUf7+z1dja/8z4De3G3ci23HYj489kST15uEsSVJvhogkqTdDRJLUmyEiSerNEJEk9WaISLsoyfdHMM9DtnvS7DuT/PZiL0faVYaItDQdQnefgrSkGSLSIkryn5Nc1R709/utbVWSm5N8LN27YL6Y5Mlt2D9u416b7r0cN7Q7pN8FvL61v77N/uAkX0lyW5LfatPvmeTSJF9r075+3sKkETFEpEWS5Ci6R1IcRrcn8aKBB1AeBPxJVT0feIDuLmSAjwP/tqoOAR4BqO7R4+8ALqqqQ6pq7jlKz6N71PthwOntWWrHAN+pql+pqhcAfznq9ZQGGSLS4jmqfa6hezzF8+jCA+BbVXVt694ErEr3BsCnVdXftPZPPs78L63uYXzfpXsQ4350j/L4Z0nOTPLSqnpwEddHelyGiLR4Avxh23s4pKqeU1XntGE/GhjvEWD3HvP/mXlU1Tfo3pB3PfCeJO/oU7jUlyEiLZ7LgN9o73whycok/2ChkavqAeDhJIe3phMHBj9M9xrkHUryLOCHVfXnwPvoAkUamz5/DUmaR1V9MckvAX/TPZmb7wP/knauYwEnAx9L8hPgr4C5w1FfBtYnuRb4wx1M/4+A97Xpfwy8edfWQto5PsVXmqAkT62q77fu9XSPKH/rhMuShuaeiDRZxyc5je538Q7g1ydbjrRz3BORJPXmiXVJUm+GiCSpN0NEktSbISJJ6s0QkST19v8BJ8ULABHHSGsAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 대부분의 리뷰 길이는 300 미만\n",
        "- 리뷰는 대부분 짧아서 중간값보다 훨씬 짧은 100개의 단어만 사용\n",
        "  - 하지만 100개의 단어보다 작은 리뷰 존재\n",
        "  - 이런 리뷰들의 길이를 맞추기 위해 패딩이 필요\n",
        "    - 보통 패딩을 나타내는 토큰 : 0"
      ],
      "metadata": {
        "id": "IaPrWMDoKZw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 케라스는 시퀀스 데이터의 길이를 맞추는 pad_squences() 함수를 제공\n",
        "- 이 함수를 사용해 train_input의 길이를 100으로 맞춤"
      ],
      "metadata": {
        "id": "UNbx103PK9QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "train_seq = pad_sequences(train_input, maxlen=100)"
      ],
      "metadata": {
        "id": "Y0_v046sMJ9T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- maxlen에 원하는 길이를 지정하면 이보다 긴 경우는 잘라내고 짧은 경우는 0으로 패딩"
      ],
      "metadata": {
        "id": "IUTz5okVMpYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_seq.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGQxmM8jMzfZ",
        "outputId": "6c243b2f-327d-438a-90c5-50e0c8d41f92"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- train_input은 파이썬 리스트의 배열이었지만, 길이를 100으로 맞춘 train_seq는 이제 (20000, 100) 크기의 2차원 배열이 되었음"
      ],
      "metadata": {
        "id": "3x8UvMDYM9Ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_seq[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z7lBqEtNUAA",
        "outputId": "73137e47-8ce6-4b28-f59d-cec84b35a5a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 10   4  20   9   2 364 352   5  45   6   2   2  33 269   8   2 142   2\n",
            "   5   2  17  73  17 204   5   2  19  55   2   2  92  66 104  14  20  93\n",
            "  76   2 151  33   4  58  12 188   2 151  12 215  69 224 142  73 237   6\n",
            "   2   7   2   2 188   2 103  14  31  10  10 451   7   2   5   2  80  91\n",
            "   2  30   2  34  14  20 151  50  26 131  49   2  84  46  50  37  80  79\n",
            "   6   2  46   7  14  20  10  10 470 158]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_input[0][-10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-hQRRMRNcd5",
        "outputId": "90d17ed2-37cb-428c-95a1-f01cdd8798de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 2, 46, 7, 14, 20, 10, 10, 470, 158]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 음수 인덱스와 슬라이싱을 사용해 train_input[0]에 있는 마지막 10개의 토큰을 출력\n",
        "- train_seq[0]의 출력값과 비교하면 정확히 일치\n",
        "  - 샘플의 앞부분이 잘렸음을 짐작"
      ],
      "metadata": {
        "id": "XuPx2O9tNoPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- pad_sequences() 함수는 기본으로 maxlen보다 긴 시퀀스의 앞부분을 자름\n",
        "  - 일반적으로 시퀀스의 뒷부분의 정보가 더 유용하리라 기대하기 때문"
      ],
      "metadata": {
        "id": "8fTFU0whSc1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 시퀀스의 뒷부분을 잘라내고 싶다면 pad_sequences() 함수의 truncating 매개변수의 값을 기본값 'pre'가 아닌 'post'로 바꾸면 됨"
      ],
      "metadata": {
        "id": "UDS5fB4ZSuWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_seq[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNqmuVd6TCYG",
        "outputId": "b9700c91-0c5f-46c6-fe43-8afdbcc3d4f1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0   0   0   0   1   2 195  19  49   2   2 190   4   2 352   2 183  10\n",
            "  10  13  82  79   4   2  36  71 269   8   2  25  19  49   7   4   2   2\n",
            "   2   2   2  10  10  48  25  40   2  11   2   2  40   2   2   5   4   2\n",
            "   2  95  14 238  56 129   2  10  10  21   2  94 364 352   2   2  11 190\n",
            "  24 484   2   7  94 205 405  10  10  87   2  34  49   2   7   2   2   2\n",
            "   2   2 290   2  46  48  64  18   4   2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 앞부분에 0이 있는 것으로 보아 이 샘플의 길이는 100이 안됨\n",
        "  - 같은 이유로 패딩 토큰은 시퀀스의 뒷부분이 아닌 앞부분에 추가\n",
        "- 시원스의 마지막에 있는 단어가 셀의 은닉 상테에 가장 큰 영향을 미치게 되므로 마지막에 패딩을 추가하는 것은 일반적으로 선호하지 않음\n",
        "  - 하지만 원한다면 pad_sequences() 함수의 padding 매개변수의 기본값인 'pre'를 'post'로 바꾸면 샘플의 뒷부분에 패딩을 추가 가능"
      ],
      "metadata": {
        "id": "Wd2bt-IvTG3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 검증 세트의 길이도 100으로 맞춤"
      ],
      "metadata": {
        "id": "jAej-NlmUEx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_seq = pad_sequences(val_input, maxlen=100)"
      ],
      "metadata": {
        "id": "s5bpifVvUKKf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 순환 신경망 만들기"
      ],
      "metadata": {
        "id": "JMV_L5GvUUXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 케라스는 여러 종류의 순환층 클래스를 제공\n",
        "- 그중 가장 간단한 것은 SimpleRNN 클래스\n",
        "- IMDB 리뷰 분류 문제는 이진 분류이므로 마지막 출력층은 1개의 뉴런을 가지고 시그모이드 활성화 함수를 사용해야 함"
      ],
      "metadata": {
        "id": "LcJ6OsP_UY1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 케라스의 Sequential 클래스로 만든 신경망 코드\n",
        "\n",
        "> Sequential 클래스가 순환 신경망을 만드는 용도?\n",
        "- Sequential 클래스는 순환 신경망뿐만 아니라 합성곱 신경망이나 일반적인 인공 신경망 모델을 모두 만들 수 있음\n",
        "- 순차 데이터(Sequential data)와 관련이 없음"
      ],
      "metadata": {
        "id": "kqLUgE0-VAh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.SimpleRNN(8, input_shape=(100, 500)))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "-TJYpztdVpgZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 첫 번째 매개변수는 사용할 뉴런의 개수로 input_shape에 입력 차원을 (100, 500)으로 지정\n",
        "- 첫 번째 차원이 100인 것은 앞에서 샘플의 길이를 100으로 지정했기 때문"
      ],
      "metadata": {
        "id": "fR3k1sl5WEYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 순환층도 당연히 활성화 함수를 사용해야 함\n",
        "- SimpleRNN 클래스의 activation 매개변수의 기본값 'tanh'로 하이퍼볼릭 탄젠트 함수를 사용\n",
        "  - 여기서는 기본값을 사용"
      ],
      "metadata": {
        "id": "MtNCUSE3Wa-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- input_shape의 두 번쩨 차원인 500인 이유\n",
        "  - 이전 섹션에서 만든 train_seq와 val_seq에는 한 가지 문제가 존재\n",
        "    - 토큰을 정수로 변환한 이 데이터를 신경망에 주입하면 큰 정수가 큰 활성화 출력을 만들기 때문\n",
        "  - 단순한 정숫값을 신경망에 입력하기 위해서는 다른 방법을 찾아야 함\n",
        "  - 정숫값에 있는 크기 속성을 없애고 각 정수를 고유하게 표현하는 방법은 원-핫 인코딩"
      ],
      "metadata": {
        "id": "CFgLLyKQZFaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- imdb.load_data() 함수에서 500개의 단어만 사용하도록 지정했기 때문에 고유한 단어는 모두 500개\n",
        "  - 훈련 데이터에 포함될 수 있는 정숫값의 범위는 0(패딩 토큰)에서 499까지\n",
        "  - 따라서 이 범위를 원-핫 인코딩으로 표현하려면 배열의 길이가 500이어야 함"
      ],
      "metadata": {
        "id": "EL0oebzecGXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰마다 500개의 숫자를 사용해 표현\n",
        "  - 다만 500개 중에 하나만 1이고 나머지는 모두 0으로 만드러 정수 사이에 있던 크기 속성을 없애는 원-핫 인코딩을 사용 "
      ],
      "metadata": {
        "id": "y34qrGJCcoZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- keras.utils 패키지 아래에 있는 to_categorical() 함수\n",
        "  - 정수 배열을 입력하면 자동으로 원-핫 인코딩된 배열을 반환"
      ],
      "metadata": {
        "id": "12vxBC72dDbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_oh = keras.utils.to_categorical(train_seq)"
      ],
      "metadata": {
        "id": "-K2hQUR4divf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_oh.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfFvcmykdqRf",
        "outputId": "85912647-fc9c-40bf-fb58-237ea4ee22ae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 정수 하나마다 모두 500차원의 배열로 변경되었기 때문에 (20000, 100) 크기의 train_seq가 (20000, 100, 500) 크기의 train_oh로 바뀜\n",
        "- 샘플 데이터의 크기가 1차원 정수 배열 (100, )에서 2차원 배열 (100, 500)로 바꿔야 하므로 SimpleRNN 클래스의 input_shape 매개변수의 값을 (100, 500)으로 지정"
      ],
      "metadata": {
        "id": "7HG1BLeSdtm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_oh[0][0][:12])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99ju0VKFeqIy",
        "outputId": "ba3afaae-7b85-4fc5-c4d4-8d325a9f5bfc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 처음 12개 원소를 출력해 보면 열한 번째 원소가 1인 것을 확인 가능"
      ],
      "metadata": {
        "id": "EPINvLdwezhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum(train_oh[0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_HotfTWe9qp",
        "outputId": "7a92471a-45ed-470e-b333-a859b61b76a9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 나머지 원소가 전부 0인 것을 확인\n",
        "- 토큰 10이 잘 인코딩됨"
      ],
      "metadata": {
        "id": "V7ZEjApVfGsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 같은 방식으로 val_seq도 원-핫 인코딩으로 변환"
      ],
      "metadata": {
        "id": "GLLgKkrdgFpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_oh = keras.utils.to_categorical(val_seq)"
      ],
      "metadata": {
        "id": "juXrI7Z8gLBR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델의 구조를 출력"
      ],
      "metadata": {
        "id": "x6IlR-uJgSm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OajT-k_gXYE",
        "outputId": "15f1f8d6-7e4e-4b33-8af3-b0a215193e57"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 8)                 4072      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,081\n",
            "Trainable params: 4,081\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- SimpleRNN에 전달할 샘플의 크기는 (100, 500)이지만 이 순환층은 마지막 타임스텝의 은닉 상태만 출력\n",
        "- 출력 크기가 순환층의 뉴런 개수와 동일한 8임을 확인"
      ],
      "metadata": {
        "id": "KpjbvYoqgi6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 순환층에 사용된 모델 파라미터의 개수를 계산\n",
        "  - 입력 토큰은 500차원의 원-핫 인코딩 배열\n",
        "  - 이 배열이 순환층의 뉴런 8개와 완전히 연결되기 때문에 총 $500 \\times 8 = 4000$개의 가중치가 있음\n",
        "  - 순환층의 은닉 상태는 다시 다음 타임스텝에 사용되기 위해 또 다른 가중치와 곱해짐\n",
        "  - 이 은닉 상태도 순환층의 뉴런과 완전히 연결되기 때문에 $8(은닉 상태 크기)\\times 8(뉴런 개수) = 64$개의 가중치가 필요\n",
        "  - 마지막으로 뉴런마다 하나의 절편이 존재\n",
        "    - $4000 + 64 + 8 = 4072$개의 모델 파라미터가 필요"
      ],
      "metadata": {
        "id": "ckf-5_kJg41R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 순환 신경망 훈련하기"
      ],
      "metadata": {
        "id": "J_-Cc1jHikrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이 예에서는 기본 RMSprop의 학습률 0.001을 사용하지 않기 위해 별도의 RMSprop 객체를 만들어 학습률을 0.001로 지정\n",
        "- 에포크 횟수를 100으로, 배치 크기를 64개로 설정"
      ],
      "metadata": {
        "id": "QBrDDgrhinaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model.compile(optimizer=rmsprop, loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5',\n",
        "                                                save_best_onlu = True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
        "                                                  restore_best_weights = True)\n",
        "history = model.fit(train_oh, train_target, epochs=100, batch_size=64, \n",
        "                    validation_data=(val_oh, val_target),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThwsiDFFi_hs",
        "outputId": "0001e61c-796c-4ff7-d91a-eeecef5169b8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 31s 85ms/step - loss: 0.7004 - accuracy: 0.4995 - val_loss: 0.7015 - val_accuracy: 0.4874\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 26s 82ms/step - loss: 0.6961 - accuracy: 0.5068 - val_loss: 0.6984 - val_accuracy: 0.4920\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.6932 - accuracy: 0.5163 - val_loss: 0.6962 - val_accuracy: 0.5020\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 35s 111ms/step - loss: 0.6910 - accuracy: 0.5275 - val_loss: 0.6946 - val_accuracy: 0.5030\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 26s 81ms/step - loss: 0.6890 - accuracy: 0.5373 - val_loss: 0.6931 - val_accuracy: 0.5146\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.6840 - accuracy: 0.5591 - val_loss: 0.6851 - val_accuracy: 0.5580\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.6760 - accuracy: 0.5914 - val_loss: 0.6787 - val_accuracy: 0.5818\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.6672 - accuracy: 0.6197 - val_loss: 0.6695 - val_accuracy: 0.6074\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.6558 - accuracy: 0.6458 - val_loss: 0.6593 - val_accuracy: 0.6304\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 26s 82ms/step - loss: 0.6429 - accuracy: 0.6683 - val_loss: 0.6447 - val_accuracy: 0.6602\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.6279 - accuracy: 0.6911 - val_loss: 0.6300 - val_accuracy: 0.6848\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 25s 79ms/step - loss: 0.6116 - accuracy: 0.7099 - val_loss: 0.6149 - val_accuracy: 0.6976\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.5950 - accuracy: 0.7234 - val_loss: 0.6011 - val_accuracy: 0.7122\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.5786 - accuracy: 0.7389 - val_loss: 0.5865 - val_accuracy: 0.7228\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.5641 - accuracy: 0.7485 - val_loss: 0.5748 - val_accuracy: 0.7270\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.5505 - accuracy: 0.7568 - val_loss: 0.5638 - val_accuracy: 0.7338\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.5375 - accuracy: 0.7653 - val_loss: 0.5531 - val_accuracy: 0.7426\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.5258 - accuracy: 0.7696 - val_loss: 0.5462 - val_accuracy: 0.7412\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.5150 - accuracy: 0.7746 - val_loss: 0.5334 - val_accuracy: 0.7546\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.5049 - accuracy: 0.7830 - val_loss: 0.5255 - val_accuracy: 0.7556\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.4956 - accuracy: 0.7857 - val_loss: 0.5191 - val_accuracy: 0.7602\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - 26s 82ms/step - loss: 0.4865 - accuracy: 0.7898 - val_loss: 0.5098 - val_accuracy: 0.7644\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4784 - accuracy: 0.7946 - val_loss: 0.5033 - val_accuracy: 0.7706\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - 26s 81ms/step - loss: 0.4716 - accuracy: 0.7967 - val_loss: 0.5000 - val_accuracy: 0.7696\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.4650 - accuracy: 0.7973 - val_loss: 0.4908 - val_accuracy: 0.7768\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4588 - accuracy: 0.8006 - val_loss: 0.4862 - val_accuracy: 0.7770\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.4529 - accuracy: 0.8044 - val_loss: 0.4854 - val_accuracy: 0.7732\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.4482 - accuracy: 0.8061 - val_loss: 0.4778 - val_accuracy: 0.7826\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4434 - accuracy: 0.8082 - val_loss: 0.4758 - val_accuracy: 0.7802\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.4396 - accuracy: 0.8094 - val_loss: 0.4740 - val_accuracy: 0.7814\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - 25s 79ms/step - loss: 0.4354 - accuracy: 0.8116 - val_loss: 0.4692 - val_accuracy: 0.7880\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.4321 - accuracy: 0.8127 - val_loss: 0.4698 - val_accuracy: 0.7818\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.4288 - accuracy: 0.8130 - val_loss: 0.4673 - val_accuracy: 0.7858\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.4257 - accuracy: 0.8148 - val_loss: 0.4639 - val_accuracy: 0.7906\n",
            "Epoch 35/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4232 - accuracy: 0.8169 - val_loss: 0.4656 - val_accuracy: 0.7866\n",
            "Epoch 36/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4201 - accuracy: 0.8168 - val_loss: 0.4631 - val_accuracy: 0.7870\n",
            "Epoch 37/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4180 - accuracy: 0.8180 - val_loss: 0.4699 - val_accuracy: 0.7840\n",
            "Epoch 38/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4159 - accuracy: 0.8206 - val_loss: 0.4611 - val_accuracy: 0.7890\n",
            "Epoch 39/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4137 - accuracy: 0.8203 - val_loss: 0.4599 - val_accuracy: 0.7900\n",
            "Epoch 40/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.4119 - accuracy: 0.8221 - val_loss: 0.4599 - val_accuracy: 0.7888\n",
            "Epoch 41/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4100 - accuracy: 0.8225 - val_loss: 0.4609 - val_accuracy: 0.7862\n",
            "Epoch 42/100\n",
            "313/313 [==============================] - 25s 81ms/step - loss: 0.4086 - accuracy: 0.8233 - val_loss: 0.4585 - val_accuracy: 0.7908\n",
            "Epoch 43/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4069 - accuracy: 0.8230 - val_loss: 0.4582 - val_accuracy: 0.7892\n",
            "Epoch 44/100\n",
            "313/313 [==============================] - 25s 79ms/step - loss: 0.4052 - accuracy: 0.8252 - val_loss: 0.4587 - val_accuracy: 0.7896\n",
            "Epoch 45/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4036 - accuracy: 0.8253 - val_loss: 0.4585 - val_accuracy: 0.7886\n",
            "Epoch 46/100\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.4028 - accuracy: 0.8252 - val_loss: 0.4623 - val_accuracy: 0.7874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "xciwDo5Ek3Ht",
        "outputId": "9bd73e5e-168f-4ad6-8455-75f9fdb97fe8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 훈련 손실은 꾸준히 감소하고 있지만 검증 손실은 대략 20번째에서 감소가 둔해짐\n",
        "- 원-핫 인코딩의 단점은 입력 데이터가 엄청나게 커진다는 것"
      ],
      "metadata": {
        "id": "LE9uBcrDpVLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_seq.nbytes, train_oh.nbytes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_uieHtvrK_S",
        "outputId": "909b60a9-137e-4ce1-9555-5fb9b729de2b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000000 4000000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰 1개를 500차원으로 늘렸기 때문에 대략 500배가 커짐\n",
        "- 훈련 데이터가 커질수록 더 문제가 됨"
      ],
      "metadata": {
        "id": "8ODulBmUraV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 단어 임베딩을 사용하기"
      ],
      "metadata": {
        "id": "RhIRXm8Fr56w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 순환 신경망에서 텍스트를 처리할 때 즐려 사용하는 방법은 단어 임베딩(word embedding)\n",
        "- 단어 임베딩은 각 단어를 고정된 크기의 실수 벡터로 바꾸어줌\n",
        "- 단어 임베딩으로 만들어진 벡터는 원-핫 인코딩된 벡터보다 훨씬 의미 있는 값으로 채워져 있기 때문에 자연어 처리에서 더 좋은 성능을 내는 경우가 많음"
      ],
      "metadata": {
        "id": "6vGIUL87sID0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- keras.layers 패키지 아래 Embedding 클래스로 임베딩 기능을 제공\n",
        "- 이 클래스를 다른 층처럼 모델에 추가하면 처음에는 모든 벡터가 랜덤하게 초기화되지만 훈련을 통해 데이터에서 좋은 단어 임베딩을 학습"
      ],
      "metadata": {
        "id": "4om_CGKctS0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어 임베딩의 장점은 입력으로 정수 데이터를 받는 것\n",
        "  - 원-핫 인코딩으로 변경된 train_oh 배열이 아니라 train_seq를 사용할 수 있음\n",
        "  - 메모리를 훨씬 효울적으로 사용 가능"
      ],
      "metadata": {
        "id": "DxkRxHOPtrFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 원-핫 인코딩은 샘플 하나를 500차원으로 늘렸기 때문에 (100, ) 크기의 샘플이 (100, 500)으로 커짐\n",
        "- 임베딩도 (100, ) 크기의 샘플을 (100, 20)과 같이 2차원 배열로 늘림\n",
        "- 하지만 원-핫 인코딩과는 달리 훨씬 작은 크기로도 단어를 잘 표현 가능"
      ],
      "metadata": {
        "id": "7_lzSQodt_o2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Embedding 클래스를 SimpleRNN 층 앞에 추가한 두 번째 순환 신경망을 생성"
      ],
      "metadata": {
        "id": "68JEz48busOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = keras.Sequential()\n",
        "model2.add(keras.layers.Embedding(500, 16, input_length=100))\n",
        "model2.add(keras.layers.SimpleRNN(8))\n",
        "model2.add(keras.layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "CMJJT1Cau_N7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Embedding 클래스의 첫 번째 매개변수 (500)는 어휘 사전의 크기\n",
        "  - 앞서 IMDB 리뷰 데이터셋에서 500개의 단어만 사용하도록 imdb.load_data(num_words=500)과 같이 설정했기 때문에 이 매개변수의 값을 500으로 지정"
      ],
      "metadata": {
        "id": "zgQ4eO79u3gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 두 번째 매개변수 (16)는 임베딩 벡터의 크기\n",
        "  - 원-핫 인코딩보다 훨씬 작은 크기 (16)의 벡터를 사용"
      ],
      "metadata": {
        "id": "PmtTmeQKv6FN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 세 번째 input_length 매개변수는 입력 시퀀스의 길이\n",
        "  - 앞서 샘플의 길이를 100으로 맞추어 train_seq를 생성\n",
        "  - 따라서 100으로 지정"
      ],
      "metadata": {
        "id": "0AZgulElwLCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- SimpleRNN 층과 Dense 층은 이전과 동일"
      ],
      "metadata": {
        "id": "toxbnJiNwbE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9daA6D2wlLL",
        "outputId": "0c1e4018-b5cf-409d-a151-c9c9cb734f46"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 16)           8000      \n",
            "                                                                 \n",
            " simple_rnn_2 (SimpleRNN)    (None, 8)                 200       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,209\n",
            "Trainable params: 8,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이 모델은 (100, ) 크기의 입력을 받아 (100, 16) 크기의 출력을 생성"
      ],
      "metadata": {
        "id": "wOFDHDLNwxIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Embedding 클래스는 500개의 각 토큰을 크기가 16인 벡터로 변경하기 때문데 총 $500 \\times 16 = 8000$개의 모델 파라미터를 가짐\n",
        "- 그 다음 SimpleRNN 층은 임베딩 벡터의 크기가 16이므로 8개의 뉴런과 곱하기 위해 필요한 가중치 $16 \\times 8 =128$개를 가짐\n",
        "- 은닉 상태에 곱해지는 $8\\times 8 = 64$개가 있음\n",
        "- 마지막으로 8개의 절편이 존재\n",
        "  - $128 + 64 + 8 = 200$개의 모델 파라미터"
      ],
      "metadata": {
        "id": "1dp1zfynw8i3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 마지막 Dense 층의 가중치 개수는 이전과 동일하게 9개\n",
        "- 원-핫 인코딩보다 SimpleRNN에 주입되는 입력의 크기가 크게 줄었지만 임베딩 벡터는 단어를 잘 표현하는 능력이 있기 때문에 훈련 결과는 이전에 못지 않음\n",
        "- 훈련 과정은 이전과 동일"
      ],
      "metadata": {
        "id": "NT0Wtjgnyiul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "model2.compile(optimizer=rmsprop, loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint('best-embedding-model.h5',\n",
        "                                                save_best_onlu = True)\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3,\n",
        "                                                  restore_best_weights = True)\n",
        "history = model2.fit(train_seq, train_target, epochs=100, batch_size=64, \n",
        "                    validation_data=(val_seq, val_target),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwGVwPBzy8gh",
        "outputId": "c0dfdb10-9e99-4da6-f813-33b752c790c9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 32s 99ms/step - loss: 0.6949 - accuracy: 0.5070 - val_loss: 0.6948 - val_accuracy: 0.5012\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 31s 98ms/step - loss: 0.6913 - accuracy: 0.5238 - val_loss: 0.6930 - val_accuracy: 0.5136\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.6883 - accuracy: 0.5418 - val_loss: 0.6916 - val_accuracy: 0.5224\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.6852 - accuracy: 0.5553 - val_loss: 0.6906 - val_accuracy: 0.5248\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 31s 100ms/step - loss: 0.6819 - accuracy: 0.5654 - val_loss: 0.6898 - val_accuracy: 0.5256\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 31s 98ms/step - loss: 0.6784 - accuracy: 0.5796 - val_loss: 0.6892 - val_accuracy: 0.5296\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 31s 101ms/step - loss: 0.6747 - accuracy: 0.5874 - val_loss: 0.6883 - val_accuracy: 0.5334\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 31s 100ms/step - loss: 0.6708 - accuracy: 0.5958 - val_loss: 0.6870 - val_accuracy: 0.5404\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.6668 - accuracy: 0.6024 - val_loss: 0.6859 - val_accuracy: 0.5408\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 31s 98ms/step - loss: 0.6625 - accuracy: 0.6105 - val_loss: 0.6848 - val_accuracy: 0.5472\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 31s 101ms/step - loss: 0.6583 - accuracy: 0.6168 - val_loss: 0.6834 - val_accuracy: 0.5520\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 31s 98ms/step - loss: 0.6538 - accuracy: 0.6270 - val_loss: 0.6815 - val_accuracy: 0.5612\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - 31s 100ms/step - loss: 0.6494 - accuracy: 0.6346 - val_loss: 0.6812 - val_accuracy: 0.5612\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - 30s 96ms/step - loss: 0.6451 - accuracy: 0.6420 - val_loss: 0.6793 - val_accuracy: 0.5644\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - 32s 102ms/step - loss: 0.6405 - accuracy: 0.6475 - val_loss: 0.6782 - val_accuracy: 0.5684\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - 31s 100ms/step - loss: 0.6358 - accuracy: 0.6561 - val_loss: 0.6747 - val_accuracy: 0.5782\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - 31s 98ms/step - loss: 0.6311 - accuracy: 0.6604 - val_loss: 0.6744 - val_accuracy: 0.5794\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - 31s 100ms/step - loss: 0.6264 - accuracy: 0.6650 - val_loss: 0.6729 - val_accuracy: 0.5852\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - 30s 97ms/step - loss: 0.6217 - accuracy: 0.6723 - val_loss: 0.6700 - val_accuracy: 0.5920\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - 31s 98ms/step - loss: 0.6171 - accuracy: 0.6750 - val_loss: 0.6671 - val_accuracy: 0.5972\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - 32s 102ms/step - loss: 0.6124 - accuracy: 0.6803 - val_loss: 0.6661 - val_accuracy: 0.5968\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - 32s 101ms/step - loss: 0.6079 - accuracy: 0.6862 - val_loss: 0.6644 - val_accuracy: 0.6008\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - 31s 98ms/step - loss: 0.6032 - accuracy: 0.6890 - val_loss: 0.6615 - val_accuracy: 0.6080\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - 31s 100ms/step - loss: 0.5988 - accuracy: 0.6916 - val_loss: 0.6604 - val_accuracy: 0.6080\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - 32s 101ms/step - loss: 0.5945 - accuracy: 0.6957 - val_loss: 0.6615 - val_accuracy: 0.6040\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.5904 - accuracy: 0.7017 - val_loss: 0.6578 - val_accuracy: 0.6156\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.5862 - accuracy: 0.7031 - val_loss: 0.6570 - val_accuracy: 0.6174\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - 30s 97ms/step - loss: 0.5820 - accuracy: 0.7053 - val_loss: 0.6583 - val_accuracy: 0.6118\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - 30s 96ms/step - loss: 0.5781 - accuracy: 0.7105 - val_loss: 0.6571 - val_accuracy: 0.6166\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - 31s 98ms/step - loss: 0.5741 - accuracy: 0.7116 - val_loss: 0.6559 - val_accuracy: 0.6184\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - 32s 101ms/step - loss: 0.5701 - accuracy: 0.7177 - val_loss: 0.6584 - val_accuracy: 0.6172\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.5664 - accuracy: 0.7200 - val_loss: 0.6549 - val_accuracy: 0.6218\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - 32s 101ms/step - loss: 0.5629 - accuracy: 0.7209 - val_loss: 0.6531 - val_accuracy: 0.6250\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - 32s 101ms/step - loss: 0.5594 - accuracy: 0.7236 - val_loss: 0.6547 - val_accuracy: 0.6234\n",
            "Epoch 35/100\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.5556 - accuracy: 0.7283 - val_loss: 0.6593 - val_accuracy: 0.6158\n",
            "Epoch 36/100\n",
            "313/313 [==============================] - 31s 99ms/step - loss: 0.5525 - accuracy: 0.7298 - val_loss: 0.6553 - val_accuracy: 0.6274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Si-BhzyRz_8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 검증 손실이 더 감소되지 않아 훈련이 적절히 조기 종료\n",
        "- 이에 비해 훈련 손실은 계속 감소"
      ],
      "metadata": {
        "id": "o-FhxWp20Plc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 마무리"
      ],
      "metadata": {
        "id": "nXecZhIK0dFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 키워드로 끝내는 핵심 포인트"
      ],
      "metadata": {
        "id": "2z7FV-_Z0ju2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 말뭉치 : 자연어 처리에서 사용하는 텍스트 데이터의모음, 즉 훈련 데이터셋\n",
        "- 토큰 : 텍스트에서 공백으로 구분되는 문자열. 종종 소문자로 변환하고 구둣점은 삭제\n",
        "- 원-핫 인코딩 : 어떤 클래스에 해당하는 원소만 1이고 나머지는 모두 0인 벡터.\n",
        "  - 정수로 변환된 토큰을 원-핫 인코딩으로 변환하려면 어휘 사전 크기의 벡터가 생성\n",
        "- 단어 임베딩 : 정수로 변환된 토큰을 비교적 작은 크기의 실수 밀집 벡터로 변환\n",
        "  - 이런 밀집 벡터는 단어 사이의 관계를 표현할 수 있기 때문에 자연어 처리에서 좋은 성능을 발휘"
      ],
      "metadata": {
        "id": "qaCqErFx0n00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 핵심 패키지와 함수"
      ],
      "metadata": {
        "id": "cKUeLrRh1dbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> TensorFlow\n",
        "- pad_sequences() : 시퀀스 길이를 맞추기 위해 패딩을 추가\n",
        "  - 이 함수는 (샘플 개수, 타임스텝 개수) 크기의 2차원 배열을 기대\n",
        "  - maxlen : 원하는 시퀀스 길이를 지정\n",
        "    - 이 값보다 긴 시퀀스는 잘리고 짧은 시퀀스는 패딩\n",
        "    - 이 매개변수를 지정하지 않으면 가장 긴 시퀀스의 길이가 됨\n",
        "  - padding : 패딩을 추가할 위치를 지정\n",
        "    - 'pre' : 기본값\n",
        "      - 시퀀스 앞에 패딩을 추가\n",
        "    - 'post' : 시퀀스 뒤에 패딩을 추가\n",
        "  - 'truncating' : 긴 시퀀에서 잘라버릴 위치를 지정\n",
        "    - 'pre' : 시퀀스 앞부분을 잘라냄\n",
        "    - 'post' : 시퀀스 뒷부분을 잘라냄\n",
        "- to_categorical() : 정수 시퀀스를 원-핫 인코딩으로 변환\n",
        "  - 토큰을 원-핫 인코딩하거나 타깃값을 원-핫 인코딩할 때 사용\n",
        "  - num_classes : 클래스 개수를 지정\n",
        "    - 지정하지 않으면 데이터에서 자동으로 찾음\n",
        "- SimpleRNN : 케라스의 기본 순환층 클래스\n",
        "  - 첫 번째 매개변수 : 뉴런의 개수를 지정\n",
        "  - activation : 매개변수에서 활성화 함수를 지정\n",
        "    - 'tanh' : 기본값, 하이퍼볼릭 탄젠트\n",
        "    - dropout : 입력에 대한 드롭아웃 비율을 지정\n",
        "    - return_sequences : 모든 타임스텝의 은닉 상태를 출력할지 결정\n",
        "      - 기본값 : False\n",
        "- Embedding : 단어 임베딩을 위한 클래스\n",
        "  - 첫 번째 매개변수 : 어휘 사전의 크기를 지정\n",
        "  - 두 번째 매개변수 : Embedding 층이 출력할 밀집 벡터의 크기를 지정\n",
        "  - input_length : 입력 시퀀스의 길이를 지정\n",
        "    - Embedding 층 바로 뒤에 Flatten이나 Dense 클래스가 올 때 꼭 필요"
      ],
      "metadata": {
        "id": "Qg5_212_1kPs"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "chapter9_2",
      "provenance": [],
      "authorship_tag": "ABX9TyPGKLAw2SnpK6lQ7zD9n2Bg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}